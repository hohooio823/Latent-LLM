{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deqian/anaconda3/envs/lan/lib/python3.8/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "\"\"\"\n",
    "Sample from the trained model with PyTorch\n",
    "\"\"\"\n",
    "import os\n",
    "import sys\n",
    "import pickle\n",
    "from contextlib import nullcontext\n",
    "import torch\n",
    "sys.path.append(os.path.dirname(os.path.dirname(os.path.dirname(\"/home/deqian/random_effect_LLM/sampling/zero-shot-reasoning-likelihood/zero_shot_our.ipynb\"))))\n",
    "import time \n",
    "from optimizer import PosteriorOptimizer\n",
    "import numpy as np\n",
    "import argparse\n",
    "import yaml\n",
    "from tokenizer import Tokenizer\n",
    "from transformers import GPT2TokenizerFast, GPT2LMHeadModel, GPTNeoXTokenizerFast\n",
    "import logging\n",
    "from model_gpt import Transformer, ModelArgs\n",
    "\n",
    "import os\n",
    "import json\n",
    "import logging\n",
    "from datetime import datetime\n",
    "import random\n",
    "from zero_shot_utils import *\n",
    "\n",
    "task_to_test = ['wsc', 'winogrande', 'siqa', 'piqa', 'obqa', 'hellaswag', 'arc_easy', 'arc_challenge']\n",
    "process_functions = []\n",
    "for task in task_to_test:\n",
    "    if task in task_functions:\n",
    "        process_functions.append(task_functions[task])\n",
    "    else:\n",
    "        raise ValueError(f\"Task '{task}' not supported.\")\n",
    "\n",
    "checkpoints_to_check = ['output/gpt/test2024_11_14_05_04_29/ckpt_200000.pt',\n",
    "                        'output/gpt/test2024_11_15_01_54_10/ckpt_174000.pt',\n",
    "                        'output/gpt/test2024_11_16_05_40_48/ckpt_192000.pt',\n",
    "                        'output/gpt/test2024_11_18_06_21_25/ckpt_244000.pt']\n",
    "checkpoint = checkpoints_to_check[0]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/deqian/anaconda3/envs/lan/lib/python3.8/site-packages/huggingface_hub/file_download.py:797: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n",
      "/tmp/ipykernel_723444/1979896280.py:29: FutureWarning: You are using `torch.load` with `weights_only=False` (the current default value), which uses the default pickle module implicitly. It is possible to construct malicious pickle data which will execute arbitrary code during unpickling (See https://github.com/pytorch/pytorch/blob/main/SECURITY.md#untrusted-models for more details). In a future release, the default value for `weights_only` will be flipped to `True`. This limits the functions that could be executed during unpickling. Arbitrary objects will no longer be allowed to be loaded via this mode unless they are explicitly allowlisted by the user via `torch.serialization.add_safe_globals`. We recommend you start setting `weights_only=True` for any use case where you don't have full control of the loaded file. Please open an issue on GitHub for any issues related to this experimental feature.\n",
      "  checkpoint_dict = torch.load(checkpoint, map_location=device)\n"
     ]
    }
   ],
   "source": [
    "\n",
    "checkpoint = f'../../{checkpoint}'\n",
    "ckpt_name = f\"logs/test_batch_{checkpoint.split('/')[-2]}_{checkpoint.split('/')[-1].split('.')[0]}\"\n",
    "logging.basicConfig(filename=f\"{ckpt_name}.log\", level=logging.INFO, format=\"%(message)s\")\n",
    "\n",
    "batch_size = 12\n",
    "\n",
    "\n",
    "tokenizer = GPT2TokenizerFast.from_pretrained('gpt2')\n",
    "tokenizer.add_special_tokens({'bos_token': '<|beginoftext|>'})\n",
    "bos_token_id = tokenizer.bos_token_id\n",
    "\n",
    "# -----------------------------------------------------------------------------\n",
    "device_id = \"cuda:0\"\n",
    "device = torch.device(device_id)\n",
    "device = device_id\n",
    "dtype = \"float32\"\n",
    "seed = 0\n",
    "torch.manual_seed(seed)\n",
    "torch.cuda.manual_seed(seed)\n",
    "torch.backends.cuda.matmul.allow_tf32 = True # allow tf32 on matmul\n",
    "torch.backends.cudnn.allow_tf32 = True # allow tf32 on cudnn\n",
    "np.random.seed(seed)\n",
    "device_type = 'cuda' if 'cuda' in device else 'cpu' # for later use in torch.autocast\n",
    "ptdtype = {'float32': torch.float32, 'bfloat16': torch.bfloat16, 'float16': torch.float16}[dtype]\n",
    "ctx = nullcontext() if device_type == 'cpu' else torch.amp.autocast(device_type=device_type, dtype=ptdtype)\n",
    "\n",
    "# init from a model saved in a specific directory\n",
    "checkpoint = '../../output/gpt/test2024_11_14_05_04_29/ckpt_200000.pt'\n",
    "checkpoint_dict = torch.load(checkpoint, map_location=device)\n",
    "gptconf = ModelArgs(**checkpoint_dict['model_args'])\n",
    "cfg = checkpoint_dict[\"config\"]\n",
    "\n",
    "model = Transformer(gptconf)\n",
    "\n",
    "state_dict = checkpoint_dict['model']\n",
    "unwanted_prefix = '_orig_mod.'\n",
    "for k,v in list(state_dict.items()):\n",
    "    if k.startswith(unwanted_prefix):\n",
    "        state_dict[k[len(unwanted_prefix):]] = state_dict.pop(k)\n",
    "model.load_state_dict(state_dict, strict=False)\n",
    "\n",
    "model.eval()\n",
    "model.to(device)\n",
    "bos_token = tokenizer.bos_token\n",
    "\n",
    "##################################################################\n",
    "message_ckpt = f\"Using checkpoint {checkpoint}\"\n",
    "logging.info(\"=\"*30)\n",
    "logging.info(message_ckpt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[38804, 67699, 70608, 37619, 7877, 83966, 1871, 73135, 2496, 47954, 24675, 31921, 99059, 797, 49811, 68755, 80782, 90535, 81857, 52489, 84665, 41504, 49866]\n"
     ]
    }
   ],
   "source": [
    "all_messages = []\n",
    "task_id = 0\n",
    "process_func = process_functions[task_id]\n",
    "if task_to_test[task_id] != 'arc_easy' and task_to_test[task_id] != 'arc_challenge':\n",
    "    # Note that arc_easy and arc_challenge have different number of options (3, 4, 5 in each question)\n",
    "    dataset_lists = [process_func()]\n",
    "else: \n",
    "    dataset_lists = process_func()\n",
    "\n",
    "all_correct = 0\n",
    "all_tested = 0\n",
    "\n",
    "for subdataset_id, all_questions_list in enumerate(dataset_lists):\n",
    "    correct = 0\n",
    "    max_length = 1024\n",
    "    logging.info(f\"{task_to_test[task_id]}: max dataset token len = {max_length}\")\n",
    "    \n",
    "    # Prepare input batch\n",
    "    all_labels = [item['label'] for item in all_questions_list]        \n",
    "    padded_inputs = []\n",
    "    padded_targets = []\n",
    "\n",
    "    for item in all_questions_list:\n",
    "        sentences = [f\"{bos_token}{s}\".strip() for s in item['sentences']]\n",
    "        tokenized_batch = [tokenizer.encode(sent, add_special_tokens=False)[:gptconf.max_seq_len] for sent in sentences]\n",
    "\n",
    "        input_padded_batch = [tok + [50256] * (max_length - len(tok)) for tok in tokenized_batch]  # Pad inputs with 50256 (valid token ID)\n",
    "        target_padded_batch = [tok + [-1] * (max_length - len(tok)) for tok in tokenized_batch]  # Pad targets with -1 since adamVIPPL uses torch.cross_entropy loss. The ignore_index is -1.\n",
    "\n",
    "        padded_inputs.append(torch.stack([torch.tensor(tok, dtype=torch.long, device=device) for tok in input_padded_batch]))\n",
    "        padded_targets.append(torch.stack([torch.tensor(tok, dtype=torch.long, device=device) for tok in target_padded_batch]))\n",
    "\n",
    "    # Group multiple batches together\n",
    "    grouped_batches = [padded_inputs[i:i + batch_size] for i in range(0, len(padded_inputs), batch_size)]\n",
    "    grouped_targets = [padded_targets[i:i + batch_size] for i in range(0, len(padded_targets), batch_size)]\n",
    "    grouped_labels = [all_labels[i:i + batch_size] for i in range(0, len(all_labels), batch_size)]\n",
    "    \n",
    "    logging.info(f\"len(grouped_batches): {len(grouped_batches)}, len(grouped_batches[0]): {len(grouped_batches[0])} , grouped_batches[0][0].shape: {grouped_batches[0][0].shape}\")\n",
    "    group_seed = [np.random.randint(100000) for i in range(len(grouped_batches))]\n",
    "    print(group_seed)\n",
    "    num_options_per_question = len(all_questions_list[0]['sentences'])\n",
    "    logging.info(f\"number of options per question: {num_options_per_question}\")\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "group 0: last one loss: 68.946, generatedID: 1, correctID: 0, current rate 0.42 (5/12)\n",
      "group 1: last one loss: 87.913, generatedID: 1, correctID: 0, current rate 0.50 (12/24)\n",
      "group 2: last one loss: 92.055, generatedID: 1, correctID: 0, current rate 0.50 (18/36)\n",
      "group 3: last one loss: 73.471, generatedID: 1, correctID: 0, current rate 0.46 (22/48)\n",
      "group 4: last one loss: 87.770, generatedID: 1, correctID: 0, current rate 0.47 (28/60)\n",
      "group 5: last one loss: 77.526, generatedID: 0, correctID: 0, current rate 0.47 (34/72)\n",
      "group 6: last one loss: 62.658, generatedID: 0, correctID: 0, current rate 0.50 (42/84)\n",
      "group 7: last one loss: 88.957, generatedID: 1, correctID: 0, current rate 0.50 (48/96)\n",
      "group 8: last one loss: 69.165, generatedID: 0, correctID: 0, current rate 0.51 (55/108)\n",
      "group 9: last one loss: 85.721, generatedID: 0, correctID: 0, current rate 0.52 (62/120)\n",
      "group 10: last one loss: 85.058, generatedID: 0, correctID: 0, current rate 0.52 (69/132)\n",
      "group 11: last one loss: 73.732, generatedID: 0, correctID: 0, current rate 0.51 (74/144)\n",
      "group 12: last one loss: 87.379, generatedID: 1, correctID: 0, current rate 0.51 (80/156)\n",
      "group 13: last one loss: 106.942, generatedID: 1, correctID: 0, current rate 0.52 (87/168)\n",
      "group 14: last one loss: 96.643, generatedID: 0, correctID: 0, current rate 0.52 (94/180)\n",
      "group 15: last one loss: 128.401, generatedID: 1, correctID: 0, current rate 0.53 (101/192)\n",
      "group 16: last one loss: 80.028, generatedID: 1, correctID: 0, current rate 0.52 (107/204)\n",
      "group 17: last one loss: 111.622, generatedID: 1, correctID: 0, current rate 0.52 (112/216)\n",
      "group 18: last one loss: 83.895, generatedID: 0, correctID: 0, current rate 0.53 (120/228)\n",
      "group 19: last one loss: 66.219, generatedID: 0, correctID: 0, current rate 0.53 (126/240)\n",
      "group 20: last one loss: 61.238, generatedID: 0, correctID: 0, current rate 0.52 (132/252)\n",
      "group 21: last one loss: 106.116, generatedID: 0, correctID: 0, current rate 0.52 (137/264)\n",
      "group 22: last one loss: 77.223, generatedID: 1, correctID: 0, current rate 0.52 (143/273)\n"
     ]
    }
   ],
   "source": [
    "import torch.nn.functional as F\n",
    "\n",
    "total_tested = 0\n",
    "with ctx:            \n",
    "    for group_index, (batch_group, target_group, label_group, batch_specific_seed) in enumerate(zip(grouped_batches, grouped_targets, grouped_labels, group_seed)):\n",
    "        x_input_batch = torch.cat([x[:, :-1] for x in batch_group], dim=0)\n",
    "        target_batch = torch.cat([t[:, 1:] for t in target_group], dim=0)\n",
    "        total_tested += len(label_group)\n",
    "        # Generate individual random values for batch_size, then repeat and reshape to match x_input_batch.shape[0]\n",
    "\n",
    "        torch.manual_seed(batch_specific_seed)\n",
    "        torch.cuda.manual_seed(batch_specific_seed)\n",
    "        logits = model(x_input_batch, target_batch)\n",
    "        loss = F.cross_entropy(logits.view(-1, logits.size(-1)), target_batch.view(-1), ignore_index=-1, reduction=\"none\")\n",
    "        loss = loss.view(x_input_batch.shape[0], -1).sum(dim=(-1))  # Sum over sequence length, keep batch dimension\n",
    "\n",
    "        loss_output = loss.cpu().detach().numpy()\n",
    "        \n",
    "        generated_answers = [label[np.argmin(loss_output[i:i + len(batch_group[i])])] for i, label in enumerate(label_group)]\n",
    "\n",
    "        for i, generated_answer in enumerate(generated_answers):\n",
    "            is_correct = generated_answer == all_questions_list[group_index*batch_size +i]['correct_index']\n",
    "            if is_correct:\n",
    "                correct += 1\n",
    "            \n",
    "        msg = f\"group {group_index}: last one loss: {loss[i]:.3f}, generatedID: {generated_answer}, correctID: {all_questions_list[group_index*batch_size +i]['correct_index']}, current rate {correct/total_tested:.2f} ({correct}/{total_tested})\"\n",
    "        logging.info(msg)\n",
    "        print(msg)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "2"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_group[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([24, 1023])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_input_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "9"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(batch_group)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "lan",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
