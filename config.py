"""
Configuration file for Latent Thought LM training.
GPT-2 scale training with all LTM features enabled.
"""
import os
from datetime import datetime

# -----------------------------------------------------------------------------
# Output and checkpointing settings
# -----------------------------------------------------------------------------
timestamp = datetime.now().strftime("%Y_%m_%d_%H_%M_%S")
out_dir = f"output/ltm_gpt2scale_{timestamp}"
eval_interval = 500
log_interval = 10
eval_iters = 200
always_save_checkpoint = True
init_from = "scratch"
ckpt_path = ''

# -----------------------------------------------------------------------------
# Dataset configuration
# -----------------------------------------------------------------------------
DATA_CACHE_DIR = "data_owt"
DATASET_NAME = "openwebtext"
DATASET_AUTO_DOWNLOAD = True
DATASET_NUM_SAMPLES = None
DATASET_TOKENIZER = "gpt2"
DATASET_NUM_WORKERS = 4
max_seq_len = 1024  # GPT-2 standard - THIS IS KEY!
vocab_size = 50258

# -----------------------------------------------------------------------------
# Model architecture - LTM at GPT-2 scale
# -----------------------------------------------------------------------------
dim = 768  # Same as GPT-2
n_layers = 12  # Same as GPT-2
n_heads = 12
n_kv_heads = 12
multiple_of = 32  # Keep your optimization
dropout = 0.1  # GPT-2 level dropout
window_size = 1024

# RWKV configuration - KEEP YOUR INNOVATIONS!
use_rwkv = True  # ‚úÖ Your RWKV architecture
use_rwkv8_ffn = True  # ‚úÖ RWKV-8 FFN
head_size = 64
rwkv_mode = "rwkv8"

# -----------------------------------------------------------------------------
# Latent variable configuration - ENHANCED FOR LONGER CONTEXT
# -----------------------------------------------------------------------------
num_steps = 16  # Your latent optimization steps
inference_method = 'adamVI'
initial_fast_lr = 0.3
final_fast_lr = 0.34
fast_lr = 0.3
n_prior_layers = 6  # Fixed from 0 to match your DiT
n_cls_tokens = 0
max_z_len = 192  # Increased for 1024 context (was 48 for 128)
z_dim = 768

# DiT prior configuration - KEEP YOUR INNOVATION!
use_dit_prior = True  # ‚úÖ Your DiT prior
dit_layers = 6
dit_heads = 8
dit_dim = 512
dit_multiple_of = 32
dit_num_timesteps = 1000
dit_beta_schedule = "linear"
dit_beta_start = 0.0001
dit_beta_end = 0.02

# -----------------------------------------------------------------------------
# SimCSE configuration - KEEP YOUR INNOVATION!
# --------------------------------------------------   , ---------------------------
use_simcse = True  # ‚úÖ Your contrastive learning
simcse_pooler_type = "avg"
simcse_temperature = 0.05
simcse_projection_dim = 256
simcse_use_projection_head = True
simcse_weight = 0.1

# -----------------------------------------------------------------------------
# Optimizer settings - GPT-2 SCALE
# -----------------------------------------------------------------------------
gradient_accumulation_steps = 64  # 2 * 64 = 128 effective batch
learning_rate = 6e-4  # GPT-2 learning rate
max_iters = 60000  # GPT-2 scale training
weight_decay = 0.1
beta1 = 0.9
beta2 = 0.95
grad_clip = 1.0

# Learning rate schedule
decay_lr = True
warmup_iters = 2000  # GPT-2 warmup
lr_decay_iters = max_iters
min_lr = 6e-5

# -----------------------------------------------------------------------------
# System and hardware settings
# -----------------------------------------------------------------------------
device = "cuda"
dtype = "bfloat16"
compile = True

# -----------------------------------------------------------------------------
# OPTIMIZATION SETTINGS - ALL YOUR FEATURES!
# -----------------------------------------------------------------------------
num_workers = 4
gradient_checkpointing = True  # Enable for longer sequences
use_optimized_rwkv = True  # ‚úÖ Your optimization
use_flash_attention = True  # ‚úÖ Your optimization
memory_optimization = True
use_kv_cache = False  # Disabled for training
use_efficient_dit_sampling = True  # ‚úÖ Your optimization
use_quantization = False
rwkv_optimization_level = 2
dit_optimization_level = 1
memory_efficient_attention = True
gradient_checkpointing_interval = 2  # Every 2 layers for memory
auto_batch_size = True 
use_liger = True  # ‚úÖ Your optimization
use_z_pos_emb = True  # ‚úÖ Your latent position embeddings

# -----------------------------------------------------------------------------
# Summary of changes from your original:
# -----------------------------------------------------------------------------
"""
KEPT ALL YOUR INNOVATIONS:
‚úÖ RWKV-8 architecture
‚úÖ Latent thought variables (z)
‚úÖ DiT prior for latent generation
‚úÖ SimCSE contrastive learning
‚úÖ All your optimizations

SCALED UP TO GPT-2 LEVEL:
üìè max_seq_len: 128 ‚Üí 1024 (8x longer)
üìè max_z_len: 48 ‚Üí 192 (scaled with sequence)
üìè batch_size: 25 ‚Üí 2 (memory constraints)
üìè gradient_accumulation: 4 ‚Üí 64 (effective batch 128)
üìè max_iters: 60K ‚Üí 600K (10x more training)
üìè learning_rate: 4e-4 ‚Üí 6e-4 (GPT-2 rate)
üìè warmup_iters: 1000 ‚Üí 2000 (GPT-2 warmup)
üìè dropout: 0.0 ‚Üí 0.1 (GPT-2 regularization)
üìè n_prior_layers: 0 ‚Üí 6 (match your DiT layers)

MEMORY ADJUSTMENTS FOR RTX 3080:
- Small batch size (2) due to latent memory overhead
- Gradient checkpointing enabled
- Checkpointing every 2 layers

This config will show whether your architecture innovations 
(RWKV + latents + DiT + SimCSE) actually help at proper scale!
"""

# Create config dict
def get_config_dict():
    """Return a dictionary containing all configuration parameters."""
    globals_dict = globals()
    return {k: v for k, v in globals_dict.items() 
            if not k.startswith("_") and 
               not callable(v) and 
               k not in ('os', 'datetime', 'timestamp')}